{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "SEED = 123\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(SEED)  # Apply the seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "INPUT_SIZE = (256, 256)  \n",
    "\n",
    "def count_files_in_directory(directory):\n",
    "    total_files = 0\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        total_files += len(files)\n",
    "    return total_files\n",
    "\n",
    "main_data_dir = r\"C:\\Users\\Josh\\Desktop\\CUDA\\skindiseases\"\n",
    "train_dir = os.path.join(main_data_dir, \"train\")\n",
    "val_dir = os.path.join(main_data_dir, \"val\")\n",
    "test_dir = os.path.join(main_data_dir, \"test\")\n",
    "\n",
    "train_files = count_files_in_directory(train_dir)\n",
    "val_files = count_files_in_directory(val_dir)\n",
    "test_files = count_files_in_directory(test_dir)\n",
    "\n",
    "print(f\"Training Dataset: {train_files}\")\n",
    "print(f\"Validation Dataset: {val_files}\")\n",
    "print(f\"Test Dataset: {test_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the training dataset to calculate mean and std, and get class labels\n",
    "train_dataset = datasets.ImageFolder(root=train_dir)\n",
    "class_n = list(train_dataset.class_to_idx.keys())  # Automatically retrieves class names from folders\n",
    "print(\"Class to label mapping:\", train_dataset.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to calculate mean and std for the dataset\n",
    "# def calculate_mean_std(loader):\n",
    "#     mean = 0.0\n",
    "#     std = 0.0\n",
    "#     total_images_count = 0\n",
    "#     for images, _ in loader:\n",
    "#         batch_samples = images.size(0)  # batch size (the last batch can have smaller size!)\n",
    "#         images = images.view(batch_samples, images.size(1), -1)  # reshape to (batch_size, channels, height * width)\n",
    "#         mean += images.mean(2).sum(0)\n",
    "#         std += images.std(2).sum(0)\n",
    "#         total_images_count += batch_samples\n",
    "\n",
    "#     mean /= total_images_count\n",
    "#     std /= total_images_count\n",
    "#     return mean, std\n",
    "\n",
    "# # Temporary transform to load the dataset without normalization for mean and std calculation\n",
    "# transform_temp = transforms.Compose([\n",
    "#     transforms.Resize(INPUT_SIZE),\n",
    "#     transforms.CenterCrop((227, 227)),\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "\n",
    "# # Load the training dataset without normalization\n",
    "# train_dataset_temp = datasets.ImageFolder(root=train_dir, transform=transform_temp)\n",
    "# train_loader_temp = DataLoader(train_dataset_temp, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# # Calculate mean and std\n",
    "# mean, std = calculate_mean_std(train_loader_temp)\n",
    "# print(f\"Calculated Mean: {mean}\")\n",
    "# print(f\"Calculated Std: {std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class KMeansSegmentation:\n",
    "    def __init__(self, n_clusters=3, overlay_alpha=0.5):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.overlay_alpha = overlay_alpha\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img_np = np.array(img)\n",
    "        img_flat = img_np.reshape((-1, 3))\n",
    "        kmeans = KMeans(n_clusters=self.n_clusters, random_state=SEED).fit(img_flat)\n",
    "        segmented_img = kmeans.cluster_centers_[kmeans.labels_].reshape(img_np.shape).astype(np.uint8)\n",
    "        blended_img = cv2.addWeighted(img_np, 1 - self.overlay_alpha, segmented_img, self.overlay_alpha, 0)\n",
    "        return Image.fromarray(blended_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLAHETransform:\n",
    "    def __init__(self, clip_limit=2.0, tile_grid_size=(8, 8)):\n",
    "        self.clip_limit = clip_limit\n",
    "        self.tile_grid_size = tile_grid_size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img_np = np.array(img)\n",
    "\n",
    "        img_lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2LAB)\n",
    "        l, a, b = cv2.split(img_lab)\n",
    "\n",
    "        clahe = cv2.createCLAHE(clipLimit=self.clip_limit, tileGridSize=self.tile_grid_size)\n",
    "        l_clahe = clahe.apply(l)\n",
    "\n",
    "        img_clahe = cv2.merge((l_clahe, a, b))\n",
    "        img_clahe = cv2.cvtColor(img_clahe, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "        return Image.fromarray(img_clahe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "MEAN = (0.6181, 0.4643, 0.4194)\n",
    "STD = (0.1927, 0.1677, 0.1617)\n",
    "\n",
    "\n",
    "# Define transformations\n",
    "transform_train = transforms.Compose([\n",
    "    CLAHETransform(clip_limit=2.0, tile_grid_size=(8, 8)),\n",
    "    transforms.RandomResizedCrop(227, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5), \n",
    "    transforms.RandomApply([KMeansSegmentation(n_clusters=3, overlay_alpha=0.5)], p=0.3),\n",
    "    transforms.RandomApply([transforms.RandomRotation(degrees=(-20, 20))], p=0.5),\n",
    "    transforms.RandomChoice([\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "        transforms.RandomGrayscale(p=0.2)\n",
    "    ]),\n",
    "    transforms.RandomApply([transforms.RandomAffine(degrees=5, translate=(0.02, 0.02), shear=2)], p=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD)\n",
    "])\n",
    "\n",
    "transform_val_test = transforms.Compose([\n",
    "\n",
    "    CLAHETransform(clip_limit=2.0, tile_grid_size=(8, 8)),\n",
    "    transforms.Resize(INPUT_SIZE),\n",
    "    transforms.CenterCrop((227,227)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD)\n",
    "])\n",
    "\n",
    "# Load the datasets with the new transforms\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform_train)\n",
    "val_dataset = datasets.ImageFolder(root=val_dir, transform=transform_val_test)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform_val_test)\n",
    "\n",
    "# Create DataLoaders for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "print(images.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to unnormalize the image for visualization\n",
    "def unnormalize(image, mean, std):\n",
    "    image = image.numpy().transpose((1, 2, 0))  \n",
    "    image = (image * std) + mean  \n",
    "    image = np.clip(image, 0, 1)  \n",
    "    return image\n",
    "\n",
    "\n",
    "# Visualize a batch of images from the train_loader\n",
    "def visualize_loader(loader, mean, std, class_names, num_images=6):\n",
    "    data_iter = iter(loader)\n",
    "    images, labels = next(data_iter)  \n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        image = unnormalize(images[i], mean, std)  \n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"Class: {class_names[labels[i]]}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_loader(train_loader, mean=MEAN, std=STD, class_names=class_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torchsummary import summary\n",
    "from torchvision.models import AlexNet_Weights\n",
    "\n",
    "# Define the model\n",
    "num_classes = 8\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=num_classes):  \n",
    "        super(AlexNet, self).__init__()\n",
    "        \n",
    "        self.model = models.alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)\n",
    "        self.model.classifier[6] = nn.Linear(4096, num_classes)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "alexnet = AlexNet(num_classes=num_classes)\n",
    "device = torch.device(\"cuda\") \n",
    "alexnet.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(alexnet.parameters(), lr=2e-4, weight_decay=1e-4) \n",
    "\n",
    "summary(alexnet, input_size=(3, 227, 227))\n",
    "print(alexnet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "PATIENCE = 10 \n",
    "MODEL = 'alexnet.pth'\n",
    "\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=PATIENCE)\n",
    "\n",
    "# Training Loop\n",
    "def train_model(model, train_loader, val_loader, epochs, criterion, optimizer, device, scheduler, patience, save_path=MODEL):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    best_val_accuracy = 0.0\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "            for inputs, labels in tepoch:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                tepoch.set_postfix(loss=running_loss / len(train_loader), accuracy=100 * correct / total)\n",
    "\n",
    "        epoch_train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Validation loop\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            with tqdm(val_loader, unit=\"batch\", leave=False) as vepoch:\n",
    "                vepoch.set_description(f\"Validation Epoch {epoch + 1}/{epochs}\")\n",
    "                for inputs, labels in vepoch:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "\n",
    "                    vepoch.set_postfix(val_loss=val_loss / len(val_loader), val_accuracy=100 * correct / total)\n",
    "\n",
    "        epoch_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        print(f'\\nEpoch [{epoch + 1}/{epochs}] - Train Loss: {epoch_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}%, '\n",
    "              f'Validation Loss: {epoch_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}%')\n",
    "\n",
    "        scheduler.step(epoch_val_loss)\n",
    "\n",
    "        # Early stopping\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_epoch = epoch + 1\n",
    "            patience_counter = 0\n",
    "            print(f\"New best validation accuracy: {val_accuracy:.4f}% at epoch {best_epoch}. Saving model...\\n\")\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "\n",
    "    print(f\"\\nTraining complete. Best validation accuracy: {best_val_accuracy:.4f}% at epoch {best_epoch}\")\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies\n",
    "\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = train_model(\n",
    "    alexnet, train_loader, val_loader,\n",
    "    EPOCHS, criterion, optimizer,\n",
    "    device, scheduler, patience=PATIENCE,\n",
    "    save_path=MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_losses(train_losses, val_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
