{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "SEED = 42\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(SEED)  # Apply the seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "INPUT_SIZE = (256, 256)  \n",
    "\n",
    "def count_files_in_directory(directory):\n",
    "    total_files = 0\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        total_files += len(files)\n",
    "    return total_files\n",
    "\n",
    "main_data_dir = r\"C:\\Users\\Josh\\Desktop\\CUDA\\splitdata\"\n",
    "train_dir = os.path.join(main_data_dir, \"train\")\n",
    "val_dir = os.path.join(main_data_dir, \"val\")\n",
    "test_dir = os.path.join(main_data_dir, \"test\")\n",
    "\n",
    "train_files = count_files_in_directory(train_dir)\n",
    "val_files = count_files_in_directory(val_dir)\n",
    "test_files = count_files_in_directory(test_dir)\n",
    "\n",
    "print(f\"Training Dataset: {train_files}\")\n",
    "print(f\"Validation Dataset: {val_files}\")\n",
    "print(f\"Test Dataset: {test_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the training dataset to calculate mean and std, and get class labels\n",
    "train_dataset = datasets.ImageFolder(root=train_dir)\n",
    "class_n = list(train_dataset.class_to_idx.keys())  # Automatically retrieves class names from folders\n",
    "print(\"Class to label mapping:\", train_dataset.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from sklearn.cluster import KMeans\n",
    "# from PIL import Image\n",
    "\n",
    "# class KMeansSegmentation:\n",
    "#     def __init__(self, n_clusters=10, p=0.3):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             n_clusters (int): Number of clusters for segmentation.\n",
    "#             p (float): Probability of applying K-means segmentation.\n",
    "#         \"\"\"\n",
    "#         self.n_clusters = n_clusters\n",
    "#         self.p = p\n",
    "\n",
    "#     def __call__(self, img):\n",
    "#         # Check if K-means segmentation should be applied\n",
    "#         if np.random.rand() > self.p:\n",
    "#             return img  # Skip segmentation with probability `1 - p`\n",
    "\n",
    "#         # Convert image to numpy array and reshape for K-means\n",
    "#         img_np = np.array(img)\n",
    "#         h, w, c = img_np.shape\n",
    "#         img_np = img_np.reshape(-1, c)\n",
    "\n",
    "#         # Apply K-means clustering\n",
    "#         kmeans = KMeans(n_clusters=self.n_clusters, random_state=0)\n",
    "#         kmeans.fit(img_np)\n",
    "#         segmented_img = kmeans.cluster_centers_[kmeans.labels_].reshape(h, w, c).astype(np.uint8)\n",
    "\n",
    "#         # Convert back to PIL image\n",
    "#         return Image.fromarray(segmented_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = (0.5, 0.5, 0.5)\n",
    "STD = (0.5, 0.5, 0.5)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    # Resize to standardize input dimensions\n",
    "    transforms.Resize(INPUT_SIZE),\n",
    "    transforms.CenterCrop((224, 224)), \n",
    "\n",
    "    # Apply segmentation after resizing\n",
    "    # KMeansSegmentation(n_clusters=10, p=0.3),\n",
    "\n",
    "    # Slight rotations with fill to avoid black edges\n",
    "    transforms.RandomApply(\n",
    "        [transforms.RandomRotation(degrees=(0, 20))], p=0.25\n",
    "    ),\n",
    "    transforms.RandomApply(\n",
    "        [transforms.RandomRotation(degrees=(0, 10))], p=0.25\n",
    "    ),\n",
    "    transforms.RandomApply(\n",
    "        [transforms.RandomRotation(degrees=(-20, 0))], p=0.25\n",
    "    ),\n",
    "    transforms.RandomApply(\n",
    "        [transforms.RandomRotation(degrees=(-10, 0))], p=0.25\n",
    "    ),\n",
    "\n",
    "  \n",
    "    # # Subtle affine transformations\n",
    "    # transforms.RandomApply([\n",
    "    #     transforms.RandomAffine(degrees=5, translate=(0.02, 0.02), shear=2)\n",
    "    # ], p=0.3),\n",
    "\n",
    "    # # Perspective distortion\n",
    "    # transforms.RandomApply([\n",
    "    #     transforms.RandomPerspective(distortion_scale=0.05, p=0.2)\n",
    "    # ], p=0.2),\n",
    "\n",
    "    # Minor Gaussian blur\n",
    "    transforms.RandomApply([\n",
    "        transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5))\n",
    "    ], p=0.4),\n",
    "\n",
    "    # Color and grayscale variations\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2)\n",
    "    ], p=0.7),  \n",
    "    \n",
    "    transforms.RandomApply([\n",
    "        transforms.RandomGrayscale(p=1.0)\n",
    "    ], p=0.2),  \n",
    "\n",
    "    # Horizontal flip\n",
    "    transforms.RandomApply([transforms.RandomHorizontalFlip()], p=0.5),\n",
    "\n",
    "    # Adjust sharpness slightly\n",
    "    transforms.RandomApply([\n",
    "        transforms.RandomAdjustSharpness(sharpness_factor=1.5)\n",
    "    ], p=0.3),\n",
    "\n",
    "    # Convert to tensor and normalize\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "transform_val_test = transforms.Compose([\n",
    "    transforms.Resize(INPUT_SIZE),\n",
    "    transforms.CenterCrop((224, 224)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD)\n",
    "])\n",
    "\n",
    "# Load the datasets with the new transforms\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform_train)\n",
    "val_dataset = datasets.ImageFolder(root=val_dir, transform=transform_val_test)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform_val_test)\n",
    "\n",
    "\n",
    "# Create DataLoaders for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "print(images.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to unnormalize the image for visualization\n",
    "def unnormalize(image, mean, std):\n",
    "    # Convert the tensor to a NumPy array and transpose dimensions to (H, W, C)\n",
    "    image = image.numpy().transpose((1, 2, 0))  \n",
    "    \n",
    "    # Unnormalize by reversing the mean and std normalization\n",
    "    image = (image * std) + mean  \n",
    "    \n",
    "    # Clip values to be between 0 and 1 for valid image display\n",
    "    image = np.clip(image, 0, 1)  \n",
    "    return image\n",
    "\n",
    "\n",
    "# Visualize a batch of images from the train_loader\n",
    "def visualize_loader(loader, mean, std, class_names, num_images=6):\n",
    "    # Get a batch of images\n",
    "    data_iter = iter(loader)\n",
    "    images, labels = next(data_iter)  \n",
    "\n",
    "    # Plot the images\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        image = unnormalize(images[i], mean, std)  \n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"Class: {class_names[labels[i]]}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Use this function to visualize a batch of images\n",
    "visualize_loader(train_loader, mean=MEAN, std=STD, class_names=class_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "\n",
    "# Define the function to load pretrained AlexNet with a modified classifier\n",
    "def get_alexnet_model(num_classes=7, pretrained=False):\n",
    "    # Load the pretrained AlexNet model\n",
    "    model = models.alexnet(pretrained=pretrained)\n",
    "    \n",
    "    # Modify the classifier's last layer to match the number of classes\n",
    "    model.classifier[6] = nn.Linear(in_features=4096, out_features=num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define number of classes for the last layer\n",
    "num_classes = 7\n",
    "alexnet = get_alexnet_model(num_classes=num_classes)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "alexnet.to(device)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the Adam optimizer\n",
    "optimizer = optim.Adam(alexnet.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "PATIENCE = 10\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=PATIENCE)\n",
    "\n",
    "# Implement early stopping\n",
    "def train_model(model, train_loader, val_loader, epochs, criterion, optimizer, device, scheduler, patience, save_path='best_model.pth'):\n",
    "    train_losses = []  \n",
    "    val_losses = []   \n",
    "    train_accuracies = []  \n",
    "    val_accuracies = []    \n",
    "\n",
    "    best_val_accuracy = 0.0  \n",
    "    best_epoch = 0  \n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            for inputs, labels in tepoch:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()  # Zero gradients\n",
    "                outputs = model(inputs)  # Forward pass\n",
    "                loss = criterion(outputs, labels)  # Compute loss\n",
    "\n",
    "                loss.backward()  # Backward pass\n",
    "                optimizer.step()  # Update weights\n",
    "\n",
    "                running_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                tepoch.set_postfix(loss=running_loss / (tepoch.n + 1), accuracy=100 * correct / total)\n",
    "\n",
    "        epoch_train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = 100 * correct / total \n",
    "        train_losses.append(epoch_train_loss)  \n",
    "        train_accuracies.append(train_accuracy)  \n",
    "\n",
    "        # Validation loop\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            with tqdm(val_loader, unit=\"batch\", leave=False) as vepoch:\n",
    "                vepoch.set_description(f\"Validation Epoch {epoch+1}/{epochs}\")\n",
    "                for inputs, labels in vepoch:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "\n",
    "                    vepoch.set_postfix(val_loss=val_loss / (vepoch.n + 1), val_accuracy=100 * correct / total)\n",
    "\n",
    "        epoch_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * correct / total \n",
    "        val_losses.append(epoch_val_loss) \n",
    "        val_accuracies.append(val_accuracy)  \n",
    "\n",
    "        print(f'\\nEpoch [{epoch+1}/{epochs}] - Train Loss: {epoch_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, '\n",
    "              f'Validation Loss: {epoch_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "        scheduler.step(epoch_val_loss)\n",
    "\n",
    "        # Early stopping\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_epoch = epoch + 1\n",
    "            patience_counter = 0\n",
    "            print(f\"New best validation accuracy: {val_accuracy:.2f}% at epoch {best_epoch}. Saving model...\\n\")\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "\n",
    "    print(f\"\\nTraining complete. Best validation accuracy: {best_val_accuracy:.2f}% at epoch {best_epoch}\")\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies\n",
    "\n",
    "# Train the model with early stopping\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = train_model(\n",
    "    alexnet, train_loader, val_loader, EPOCHS, criterion, optimizer, device, scheduler, patience=PATIENCE, save_path='alexnet_best_model.pth'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "def evaluate_model_on_validation(model, val_loader, device, class_names):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted classes\n",
    "            \n",
    "            # Append true labels and predictions\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Generate classification report\n",
    "    print(\"Validation Classification Report:\\n\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Plot confusion matrix using seaborn\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Validation Confusion Matrix')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()\n",
    "\n",
    "# List of class names (categories)\n",
    "class_names = class_n\n",
    "\n",
    "# Evaluate the model and generate metrics for the validation set\n",
    "evaluate_model_on_validation(alexnet, val_loader, device, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device, class_names):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    images_to_plot = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            # Append true labels and predictions\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "            # Store images and corresponding labels for plotting later\n",
    "            images_to_plot.append((inputs.cpu(), labels.cpu(), predicted.cpu()))\n",
    "\n",
    "    # Generate classification report\n",
    "    print(\"Classification Report:\\n\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Plot confusion matrix using seaborn\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()\n",
    "\n",
    "# List of class names (categories)\n",
    "class_names = class_n  # Use the defined categories\n",
    "\n",
    "# Evaluate the model and generate metrics\n",
    "evaluate_model(alexnet, test_loader, device, class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
