{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "SEED = 123\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "INPUT_SIZE = (227, 227)\n",
    "MEAN = (0.5960, 0.4489, 0.4046)\n",
    "STD = (0.2102, 0.1782, 0.1719)\n",
    "\n",
    "main_data_dir = r\"\"\n",
    "val_dir = os.path.join(main_data_dir, \"val\")\n",
    "test_dir = os.path.join(main_data_dir, \"test\")\n",
    "\n",
    "val_dataset = datasets.ImageFolder(root=val_dir)\n",
    "class_names = list(val_dataset.classes)\n",
    "print(\"Class to label mapping:\", val_dataset.class_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLAHETransform:\n",
    "    def __init__(self, clip_limit=2.0, tile_grid_size=(8, 8)):\n",
    "        self.clip_limit = clip_limit\n",
    "        self.tile_grid_size = tile_grid_size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img_np = np.array(img)\n",
    "\n",
    "        img_lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2LAB)\n",
    "        l, a, b = cv2.split(img_lab)\n",
    "\n",
    "        clahe = cv2.createCLAHE(clipLimit=self.clip_limit, tileGridSize=self.tile_grid_size)\n",
    "        l_clahe = clahe.apply(l)\n",
    "\n",
    "        img_clahe = cv2.merge((l_clahe, a, b))\n",
    "        img_clahe = cv2.cvtColor(img_clahe, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "        return Image.fromarray(img_clahe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_val_test = transforms.Compose([\n",
    "    transforms.Resize(INPUT_SIZE),\n",
    "    CLAHETransform(clip_limit=2.0, tile_grid_size=(8, 8)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD)\n",
    "])\n",
    "\n",
    "val_dataset = datasets.ImageFolder(root=val_dir, transform=transform_val_test)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform_val_test)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset,  shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=len(class_names)):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.model = models.alexnet(weights=None) \n",
    "        self.model.classifier[6] = nn.Linear(4096, num_classes)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AlexNet(num_classes=len(class_names)).to(device)\n",
    "\n",
    "model_path = r\"\"\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "model.eval() \n",
    "\n",
    "print(f\"Loaded model from {model_path}\")\n",
    "print(model)\n",
    "\n",
    "# from torchsummary import summary\n",
    "# input_size = (3, 227, 227)\n",
    "# summary(model, input_size=input_size, device=str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, device, class_names, set_name, export_txt=False):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    image_paths = []  # To store image file paths\n",
    "    confidence_scores = []  # To store confidence percentages\n",
    "\n",
    "    dataset = data_loader.dataset\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, labels) in enumerate(tqdm(data_loader, desc=f\"Evaluating {set_name} Set\")):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)  # Get raw logits\n",
    "            \n",
    "            # Convert logits to probabilities using softmax\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            \n",
    "            # Get the predicted class and the associated confidence score\n",
    "            max_probs, predicted = torch.max(probabilities, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            confidence_scores.extend(max_probs.cpu().numpy() * 100)  # Convert to percentage\n",
    "\n",
    "            # Collect image file paths from the dataset\n",
    "            image_paths.extend([dataset.samples[i][0] for i in range(idx * data_loader.batch_size, (idx + 1) * data_loader.batch_size)])\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"\\n{set_name} Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    print(f\"\\n{set_name} Classification Report:\\n\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'{set_name} Confusion Matrix on AlexNet')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()\n",
    "\n",
    "    # Export predictions to a text file if export_txt is True\n",
    "    if export_txt:\n",
    "        output_file = f\"{set_name}_alexnet_predictions_with_confidence.txt\"\n",
    "        with open(output_file, \"w\") as file:\n",
    "            file.write(\"Image File, True Label, Predicted Label, Confidence (%)\\n\")\n",
    "            for img_path, true_label, pred_label, confidence in zip(image_paths, y_true, y_pred, confidence_scores):\n",
    "                true_class_name = class_names[true_label]\n",
    "                pred_class_name = class_names[pred_label]\n",
    "                file.write(f\"{img_path}, {true_class_name}, {pred_class_name}, {confidence:.2f}%\\n\")\n",
    "        print(f\"Predictions saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, val_loader, device, class_names, \"Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, test_loader, device, class_names, \"Test\", export_txt=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tool",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
