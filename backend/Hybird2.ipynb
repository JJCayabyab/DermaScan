{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "SEED = 42\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(SEED)  # Apply the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the categories and the paths to datasets\n",
    "checkpoint_path = r'C:\\Users\\Josh\\Desktop\\CUDA\\52.41.pth'\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "INPUT_SIZE = (256, 256)\n",
    "\n",
    "def count_files_in_directory(directory):\n",
    "    total_files = 0\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        total_files += len(files)\n",
    "    return total_files\n",
    "\n",
    "main_data_dir = r\"C:\\Users\\Josh\\Desktop\\CUDA\\splitdata\"\n",
    "train_dir = os.path.join(main_data_dir, \"train\")\n",
    "val_dir = os.path.join(main_data_dir, \"val\")\n",
    "test_dir = os.path.join(main_data_dir, \"test\")\n",
    "\n",
    "train_files = count_files_in_directory(train_dir)\n",
    "val_files = count_files_in_directory(val_dir)\n",
    "test_files = count_files_in_directory(test_dir)\n",
    "\n",
    "print(f\"Training Dataset: {train_files}\")\n",
    "print(f\"Validation Dataset: {val_files}\")\n",
    "print(f\"Test Dataset: {test_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the training dataset to calculate mean and std, and get class labels\n",
    "train_dataset = datasets.ImageFolder(root=train_dir)\n",
    "class_n = list(train_dataset.class_to_idx.keys())  # Automatically retrieves class names from folders\n",
    "print(\"Class to label mapping:\", train_dataset.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = (0.5, 0.5, 0.5)\n",
    "STD = (0.5, 0.5, 0.5)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    # Resize to standardize input dimensions\n",
    "    transforms.Resize(INPUT_SIZE),\n",
    "    transforms.CenterCrop((224, 224)), \n",
    "\n",
    "    # Apply segmentation after resizing\n",
    "    # KMeansSegmentation(n_clusters=10, p=0.3),\n",
    "\n",
    "    # Slight rotations with fill to avoid black edges\n",
    "    transforms.RandomApply(\n",
    "        [transforms.RandomRotation(degrees=(0, 20))], p=0.25\n",
    "    ),\n",
    "    transforms.RandomApply(\n",
    "        [transforms.RandomRotation(degrees=(0, 10))], p=0.25\n",
    "    ),\n",
    "    transforms.RandomApply(\n",
    "        [transforms.RandomRotation(degrees=(-20, 0))], p=0.25\n",
    "    ),\n",
    "    transforms.RandomApply(\n",
    "        [transforms.RandomRotation(degrees=(-10, 0))], p=0.25\n",
    "    ),\n",
    "\n",
    "  \n",
    "    # # Subtle affine transformations\n",
    "    # transforms.RandomApply([\n",
    "    #     transforms.RandomAffine(degrees=5, translate=(0.02, 0.02), shear=2)\n",
    "    # ], p=0.3),\n",
    "\n",
    "    # # Perspective distortion\n",
    "    # transforms.RandomApply([\n",
    "    #     transforms.RandomPerspective(distortion_scale=0.05, p=0.2)\n",
    "    # ], p=0.2),\n",
    "\n",
    "    # Minor Gaussian blur\n",
    "    transforms.RandomApply([\n",
    "        transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5))\n",
    "    ], p=0.4),\n",
    "\n",
    "    # Color and grayscale variations\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2)\n",
    "    ], p=0.7),  \n",
    "    \n",
    "    transforms.RandomApply([\n",
    "        transforms.RandomGrayscale(p=1.0)\n",
    "    ], p=0.2),  \n",
    "\n",
    "    # Horizontal flip\n",
    "    transforms.RandomApply([transforms.RandomHorizontalFlip()], p=0.5),\n",
    "\n",
    "    # Adjust sharpness slightly\n",
    "    transforms.RandomApply([\n",
    "        transforms.RandomAdjustSharpness(sharpness_factor=1.5)\n",
    "    ], p=0.3),\n",
    "\n",
    "    # Convert to tensor and normalize\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "transform_val_test = transforms.Compose([\n",
    "    transforms.Resize(INPUT_SIZE),\n",
    "    transforms.CenterCrop((224, 224)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD)\n",
    "])\n",
    "\n",
    "# Load the datasets with the new transforms\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform_train)\n",
    "val_dataset = datasets.ImageFolder(root=val_dir, transform=transform_val_test)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform_val_test)\n",
    "\n",
    "\n",
    "# Create DataLoaders for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of images and labels from the DataLoader\n",
    "images, labels = next(iter(train_loader))\n",
    "print(images.shape)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to unnormalize the image for visualization\n",
    "def unnormalize(image, mean, std):\n",
    "    # Convert the tensor to a NumPy array and transpose dimensions to (H, W, C)\n",
    "    image = image.numpy().transpose((1, 2, 0))  \n",
    "    \n",
    "    # Unnormalize by reversing the mean and std normalization\n",
    "    image = (image * std) + mean  \n",
    "    \n",
    "    # Clip values to be between 0 and 1 for valid image display\n",
    "    image = np.clip(image, 0, 1)  \n",
    "    return image\n",
    "\n",
    "\n",
    "# Visualize a batch of images from the train_loader\n",
    "def visualize_loader(loader, mean, std, class_names, num_images=6):\n",
    "    # Get a batch of images\n",
    "    data_iter = iter(loader)\n",
    "    images, labels = next(data_iter)  \n",
    "\n",
    "    # Plot the images\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        image = unnormalize(images[i], mean, std)  \n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"Class: {class_names[labels[i]]}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Use this function to visualize a batch of images\n",
    "visualize_loader(train_loader, mean=MEAN, std=STD, class_names=class_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AlexNetFC6(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNetFC6, self).__init__()\n",
    "        alexnet = models.alexnet(pretrained=False)\n",
    "        self.features = alexnet.features\n",
    "        self.pooling = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.fc6 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.pooling(x) \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "alexnet_fc6 = AlexNetFC6()\n",
    "alexnet_fc6.load_state_dict(torch.load(checkpoint_path), strict=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "alexnet_fc6.to(device)\n",
    "alexnet_fc6.eval()  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data_loader, model):\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for images, target_labels in data_loader:\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Extract features using the custom AlexNet model (up to fc6)\n",
    "            output_features = model(images)\n",
    "            \n",
    "            # Append extracted features and labels to the list\n",
    "            features.append(output_features.cpu().numpy())  # Keeping this as CPU NumPy\n",
    "            labels.append(target_labels.cpu().numpy())\n",
    "    \n",
    "    # Concatenate features and labels from all batches\n",
    "    features = np.concatenate(features, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from the datasets\n",
    "train_features, y_train = extract_features(train_loader, alexnet_fc6)\n",
    "val_features, y_val = extract_features(val_loader, alexnet_fc6)\n",
    "test_features, y_test = extract_features(test_loader, alexnet_fc6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features)\n",
    "val_features_scaled = scaler.transform(val_features)\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "\n",
    "# Save the scaler model for later use\n",
    "joblib.dump(scaler, 'scaler.joblib')\n",
    "\n",
    "# Print shapes to confirm dimensions remain the same\n",
    "print(f\"Shape of train_features after scaling: {train_features_scaled.shape}\")\n",
    "print(f\"Shape of val_features after scaling: {val_features_scaled.shape}\")\n",
    "print(f\"Shape of test_features after scaling: {test_features_scaled.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost classifier\n",
    "dtrain = xgb.DMatrix(train_features_scaled, label=y_train)\n",
    "dval = xgb.DMatrix(val_features_scaled, label=y_val)\n",
    "dtest = xgb.DMatrix(test_features_scaled, label=y_test)\n",
    "\n",
    "# Verify if DMatrix creation was successful\n",
    "print(f\"Train DMatrix: {dtrain.num_row()} rows, {dtrain.num_col()} columns\")\n",
    "print(f\"Validation DMatrix: {dval.num_row()} rows, {dval.num_col()} columns\")\n",
    "print(f\"Test DMatrix: {dtest.num_row()} rows, {dtest.num_col()} columns\")\n",
    "\n",
    "# Check class distribution in training data\n",
    "from collections import Counter\n",
    "print(f\"Training class distribution: {Counter(y_train)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define the objective function using xgb.train with DMatrix\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-1),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 6),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 0.9),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.5, 0.9),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n",
    "        'gamma': trial.suggest_uniform('gamma', 0, 5),\n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-2, 100),\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-2, 100),\n",
    "        'objective': 'multi:softmax',\n",
    "        'num_class': len(class_n),\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'predictor': 'gpu_predictor',\n",
    "    }\n",
    "\n",
    "    # Use xgb.train with DMatrix\n",
    "    evals = [(dtrain, 'train'), (dval, 'validation')]\n",
    "    model = xgb.train(param, dtrain, num_boost_round=1000, evals=evals,\n",
    "                      early_stopping_rounds=20, verbose_eval=True)\n",
    "    \n",
    "    val_preds = model.predict(dval)\n",
    "    val_preds = val_preds.astype(int)  # Convert to int for compatibility with y_val\n",
    "    val_accuracy = accuracy_score(y_val, val_preds)\n",
    "    return val_accuracy\n",
    "\n",
    "# Optimize hyperparameters with Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Train a final model with the best hyperparameters using DMatrix\n",
    "best_params = study.best_params\n",
    "final_model = xgb.train(\n",
    "    best_params,\n",
    "    dtrain,\n",
    "    num_boost_round=1000,\n",
    "    evals=[(dtrain, 'train'), (dval, 'validation')],\n",
    "    early_stopping_rounds=20,\n",
    "    verbose_eval=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and evaluate on the training set\n",
    "train_predictions = final_model.predict(dtrain)\n",
    "train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "print(f'Training Accuracy: {train_accuracy * 100:.2f}%')\n",
    "\n",
    "# Predict and evaluate on the validation set\n",
    "val_predictions = final_model.predict(dval)\n",
    "val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "print(f'Validation Accuracy: {val_accuracy * 100:.2f}%')\n",
    "\n",
    "# Predict and evaluate on the test set\n",
    "test_predictions = final_model.predict(dtest)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Generate and print classification reports for validation and test sets\n",
    "val_classification_report = classification_report(y_val, val_predictions, target_names=class_n)\n",
    "print(\"Validation Classification Report:\\n\", val_classification_report)\n",
    "\n",
    "cm = confusion_matrix(y_val, val_predictions)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_n, yticklabels=class_n)\n",
    "\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "test_classification_report = classification_report(y_test, test_predictions, target_names=class_n)\n",
    "print(\"Test Classification Report:\\n\", test_classification_report)\n",
    "\n",
    "cm = confusion_matrix(y_test, test_predictions)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_n, yticklabels=class_n)\n",
    "\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
